{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this project i gathered data from 3 different resources with 3 different methods. \n",
    "like ..\n",
    "> - Importing data via csv\n",
    "> - Using requests to download data from twitter\n",
    "> - Scrape data from an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These Data are\n",
    "> - Enhanced Twitter Archive\n",
    "> - Image Predictions File\n",
    "> - Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-Enhanced Twitter Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WeRateDogs Twitter archive provided by Udacity. This contains the tweets data , i downloadit manually then read it as csv in Archive_df.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-Image Predictions File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet image predictions is present in each tweet according to a neural network. This file (image_predictions.tsv) is hosted on Udacity's servers and i downloaded programmatically using the Requests library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually twitter didn't send me any news about my application for a developer acount.\n",
    "so i had download the api twitter file provided by udacity then i read it line by line.\n",
    "but i included the query code of the query as a comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Visually, I\tused printing the three entire dataframe separate in Jupyter Notebook.\n",
    "> - programatically i used pandas methods like info,dtypes,describe and shape.\n",
    "\n",
    "then i identified problems in data like quality issues and tidiness issues then i wrote them in wrangle_act file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - each cleaning step has a three parts : define, code and Test\n",
    "> - i had to copy the three dataframes\n",
    "> - this was the the most tricky part of all wrangling project because i there was some easy issues to be fixed like changing data types, drop columns or change some values.\n",
    "> -  but there was some difficult parts like get the right ratings from the tweets text or melting columns which needed some deep search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
